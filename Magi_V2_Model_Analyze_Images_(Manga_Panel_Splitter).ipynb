{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOVWA2Q+tSYYdQFncqHF558",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curlos/manga-panel-splitter/blob/main/Magi_V2_Model_Analyze_Images_(Manga_Panel_Splitter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lCw9MUarnqyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eRJQAOPwsmrS",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92e40b24-c3ea-4781-b218-934c55a212b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.40 in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: pulp in /usr/local/lib/python3.10/dist-packages (2.9.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.40 pulp pyngrok python-dotenv accelerate psutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "import numpy as np\n",
        "from transformers import AutoModel\n",
        "import torch\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import base64\n",
        "import pdb\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "import socket\n",
        "import subprocess\n",
        "import psutil\n",
        "import logging\n",
        "import sys\n",
        "import signal\n",
        "import time\n",
        "\n",
        "def load_env_and_set_ngrok_authtoken() -> None:\n",
        "  '''\n",
        "  Load the env variables from Google Drive and set the ngrok auth token.\n",
        "  '''\n",
        "  if not os.path.ismount('/content/drive'):\n",
        "      drive.mount('/content/drive')\n",
        "\n",
        "  env_path = '/content/drive/MyDrive/google_colab_env_files/.env'\n",
        "  load_dotenv(env_path)\n",
        "\n",
        "  ngrok_auth_token = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "\n",
        "  if not ngrok_auth_token:\n",
        "      raise ValueError(\"NGROK_AUTH_TOKEN is not set. Check your .env file in Google Drive ('google_colab_env_files/.env').\")\n",
        "\n",
        "  ngrok.set_auth_token(ngrok_auth_token)\n",
        "\n",
        "load_env_and_set_ngrok_authtoken()\n",
        "\n",
        "port = 8888\n",
        "\n",
        "\"\"\"\n",
        "Configure Flask logging to use sys.stdout. This logging will only work if the Colab code block is always running.\n",
        "However, it usually won't always be running so this usually will have no visible effect.\n",
        "\"\"\"\n",
        "logging.basicConfig(\n",
        "    stream=sys.stdout,\n",
        "    level=logging.DEBUG,\n",
        "    format=\"%(asctime)s [%(levelname)s]: %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger()\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "\"\"\"\n",
        "TODO: Possibly for the future but if I want to ever send all of the pages in a chapter (like 15-20 pages), -\n",
        "- then I'd have to increase the max MB to something like 100MB.\n",
        "The reason I didn't do this here is because that seems really large for a -\n",
        "- single Flask request so that's why I'm instead currently only sending in one page at a time per request.\n",
        "I'll probably keep it to 16MB as that seems safest but definitely something to look out for in the future.\n",
        "\"\"\"\n",
        "app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # Increase limit to 16MB\n",
        "\n",
        "# Initialize the Magi model\n",
        "# TODO: In the future, try to run this with \"cuda()\".\n",
        "# In theory, it would help the NVIDIA GPUs on Google Colab run at their best and so it would speed things up.\n",
        "# I tried running this here earlier but unfortunately I kept getting \"500\" errors from the Flask HTTP request.\n",
        "# But this is definitely something that'd be worth it if it worked.\n",
        "# One additional note: When a GPU or CPU is connected to, all previous downloads are deleted.\n",
        "# So, when running this file for the first time on a freshly connected CPU or GPU, -\n",
        "# - this will have to download the Magi Model pytorch bin file again (It's around 2GB).\n",
        "magi_model = AutoModel.from_pretrained(\n",
        "            \"ragavsachdeva/magiv2\", trust_remote_code=True).eval()\n",
        "\n",
        "def get_per_page_results(magi_model, chapter_pages, character_bank):\n",
        "  \"\"\"\n",
        "  Run the predictions for the panels, texts, characters, etc. on the passed in array of chapter pages.\n",
        "  This is where the biggest calculations are done and the biggest factor in the speed of the program.\n",
        "  \"\"\"\n",
        "  # Set to \"no_grad()\" so that there's inference without tracking gradients.\n",
        "  # Basically, this saves memory and computational resources by turning off gradient tracking.\n",
        "  with torch.no_grad():\n",
        "      per_page_results = magi_model.do_chapter_wide_prediction(\n",
        "          chapter_pages, character_bank, use_tqdm=True, do_ocr=True\n",
        "      )\n",
        "\n",
        "  return per_page_results\n",
        "\n",
        "def is_port_in_use(port):\n",
        "  \"\"\"\n",
        "  Check if a port is already in use.\n",
        "  \"\"\"\n",
        "  with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "      return s.connect_ex((\"0.0.0.0\", port)) == 0\n",
        "\n",
        "def kill_process_on_port(port):\n",
        "  try:\n",
        "      # Find the process ID (PID) using netstat and taskkill\n",
        "      result = subprocess.check_output(\n",
        "          f\"netstat -ano | findstr :{port}\", shell=True\n",
        "      ).decode()\n",
        "      pid = int(result.strip().split()[-1])\n",
        "\n",
        "      # Kill the process\n",
        "      subprocess.call([\"taskkill\", \"/F\", \"/PID\", str(pid)])\n",
        "      print(f\"Port {port} is now free (process {pid} killed).\")\n",
        "  except subprocess.CalledProcessError:\n",
        "      print(f\"No process is using port {port}.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error freeing port {port}: {e}\")\n",
        "\n",
        "@app.route('/process-images-with-magi-model', methods=['POST'])\n",
        "def process_images_with_magi_model():\n",
        "  logger.info(\"Received request to /process-images-with-magi-model\")\n",
        "\n",
        "  try:\n",
        "    # Parse JSON payload\n",
        "    request_data = request.json\n",
        "    encoded_arrays = request_data.get(\"chapter_pages_image_numpy_array\")\n",
        "    character_bank = request_data.get(\"character_bank\")\n",
        "\n",
        "    # Decode and reconstruct the arrays\n",
        "    chapter_pages_image_numpy_array = [\n",
        "        np.frombuffer(base64.b64decode(item[\"data\"]), dtype=item[\"dtype\"]).reshape(item[\"shape\"])\n",
        "        for item in encoded_arrays\n",
        "    ]\n",
        "\n",
        "    # Run Magi model on the file\n",
        "    per_page_results = get_per_page_results(\n",
        "        magi_model, chapter_pages_image_numpy_array, character_bank\n",
        "    )\n",
        "\n",
        "    return per_page_results\n",
        "  except Exception as e:\n",
        "    return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/hello-world', methods=['GET'])\n",
        "def hello_world():\n",
        "  logger.info(\"Hello World endpoint hit\")\n",
        "  return 'Hello World!'\n",
        "\n",
        "def run_flask():\n",
        "  logger.info(\"Starting Flask server...\")\n",
        "  # Run Flask app without reloader (important for threading)\n",
        "  app.run(host=\"0.0.0.0\", port=port, debug=True, use_reloader=False)\n",
        "\n",
        "\n",
        "def force_free_port(port):\n",
        "    \"\"\"\n",
        "    Forcefully free a port by identifying and killing the process using it,\n",
        "    while avoiding critical processes like Colab kernel processes.\n",
        "    This is necessary because without this, I'd have to keep changing the port\n",
        "    or manually kill them myself which is a pain in the butt.\n",
        "    \"\"\"\n",
        "    for conn in psutil.net_connections(kind=\"inet\"):\n",
        "        if conn.laddr.port == port:\n",
        "            try:\n",
        "                # Get the process using the port\n",
        "                process = psutil.Process(conn.pid)\n",
        "                process_name = process.name().lower()\n",
        "                process_cmdline = \" \".join(process.cmdline())\n",
        "\n",
        "                # Skip Colab kernel-related processes\n",
        "                if \"colab\" in process_name or \"python\" in process_name and \"kernel\" in process_cmdline:\n",
        "                    print(f\"Skipping Colab or kernel-related process: {process_name} (PID {conn.pid})\")\n",
        "                    continue\n",
        "\n",
        "                # Kill the process\n",
        "                print(f\"Killing process using port {port}: {process_name} (PID {conn.pid})\")\n",
        "                os.kill(conn.pid, signal.SIGKILL)\n",
        "                print(f\"Successfully killed process {process_name} (PID {conn.pid}).\")\n",
        "            except psutil.NoSuchProcess:\n",
        "                print(f\"No such process exists for PID {conn.pid}.\")\n",
        "            except psutil.AccessDenied:\n",
        "                print(f\"Permission denied to kill process using port {port}: PID {conn.pid}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to kill process on port {port}: {e}\")\n",
        "            return\n",
        "    print(f\"Port {port} is not in use.\")\n",
        "\n",
        "def stop_all_tunnels():\n",
        "  \"\"\"\n",
        "  This was written to try to programatically stop all active NGROK tunnels programatically\n",
        "  instead of me having to go to my NGROK account and manually stopping it the active tunnels/agents.\n",
        "  However, this didn't really work. I've kept the code here just in case I want to try again later\n",
        "  but this is not currently in use anywhere on this file.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Fetch all active tunnels\n",
        "    response = requests.get(f\"http://127.0.0.1:{port}/api/tunnels\")\n",
        "    tunnels = response.json().get(\"tunnels\", [])\n",
        "\n",
        "    # Terminate each tunnel\n",
        "    for tunnel in tunnels:\n",
        "        tunnel_name = tunnel[\"name\"]\n",
        "        delete_url = f\"http://127.0.0.1:{port}/api/tunnels/{tunnel_name}\"\n",
        "        requests.delete(delete_url)\n",
        "        print(f\"Terminated tunnel: {tunnel_name}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to terminate tunnels: {e}\")\n",
        "\n",
        "def monitor_logs():\n",
        "  '''\n",
        "  For statements to be logged to the console, this loop needs to be present and run forever.\n",
        "  However, this loop will slow the Magi Model 10x.\n",
        "  So, if one page would take on average 10 seconds, -\n",
        "  it would now take around 100 seconds to compute.\n",
        "  Only use this in case of an emergency where I encounter an error and absolutely\n",
        "  must know where it's coming from or what caused it.\n",
        "  The logger should be able to tell me what happened - it'll just be super slow.\n",
        "  '''\n",
        "  while True:\n",
        "    pass\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Check and free the port if in use\n",
        "  force_free_port(port)\n",
        "\n",
        "  # Start Flask in a thread to prevent blocking.\n",
        "  # This is necessary because once I start the server, I need to expose the server to the internet use Ngrok.\n",
        "  # However, if the server is running on the main thread first, then the code block will run forever.\n",
        "  # And the problem with that is that it'll stop at that Flask line where it starts the server -\n",
        "  # - and never go to the next line of code where ngrok connects the port so I never get to have access to the Flask server publicly\n",
        "  # which means that my \"main.py\" local file can't send requests to the Colab Flask server unless I put it in a background thread\n",
        "  # so it doesn't block ngrok.\n",
        "  thread = threading.Thread(target=run_flask)\n",
        "  thread.start()\n",
        "\n",
        "  # Expose the Flask app to the internet using ngrok\n",
        "  public_url = ngrok.connect(port)\n",
        "  print(\"Public URL:\", public_url)\n",
        "\n",
        "  # This is where the monitor logs portion would come in if I need to monitor logs\n",
        "  # BUT this should only be uncommented out in case of emergency!\n",
        "  # monitor_logs()"
      ],
      "metadata": {
        "id": "spWkLnG1CATy",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6420d339-4aa0-477a-9570-ea24043587eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping Colab or kernel-related process: python3 (PID 4280)\n",
            "Port 8888 is not in use.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Address already in use\n",
            "Port 8888 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://93c1-34-125-70-89.ngrok-free.app\" -> \"http://localhost:8888\"\n"
          ]
        }
      ]
    }
  ]
}