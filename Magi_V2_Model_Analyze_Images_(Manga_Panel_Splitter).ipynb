{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOirccwRaXgnD3t9cPkoU3Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curlos/manga-panel-splitter/blob/main/Magi_V2_Model_Analyze_Images_(Manga_Panel_Splitter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lCw9MUarnqyD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRJQAOPwsmrS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.40 pulp pyngrok python-dotenv accelerate psutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f ngrok"
      ],
      "metadata": {
        "id": "ZYtUBVUwa6IY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from flask import Flask, request, jsonify\n",
        "import os\n",
        "import numpy as np\n",
        "from transformers import AutoModel\n",
        "import torch\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import base64\n",
        "import pdb\n",
        "from google.colab import drive\n",
        "from dotenv import load_dotenv\n",
        "import socket\n",
        "import subprocess\n",
        "import psutil\n",
        "import logging\n",
        "import sys\n",
        "import signal\n",
        "\n",
        "# Mount Google Drive if it hasn't mounted already.\n",
        "if not os.path.ismount('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Path to the .env file in your Drive\n",
        "env_path = '/content/drive/MyDrive/google_colab_env_files/.env'\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv(env_path)\n",
        "\n",
        "# Access the ngrok auth token\n",
        "ngrok_auth_token = os.getenv(\"NGROK_AUTH_TOKEN\")\n",
        "\n",
        "if not ngrok_auth_token:\n",
        "    raise ValueError(\"NGROK_AUTH_TOKEN is not set. Check your .env file in Google Drive ('google_colab_env_files/.env').\")\n",
        "\n",
        "ngrok.set_auth_token(ngrok_auth_token)\n",
        "\n",
        "port = 8888\n",
        "\n",
        "# Configure Flask logging to use sys.stdout\n",
        "logging.basicConfig(\n",
        "    stream=sys.stdout,\n",
        "    level=logging.DEBUG,\n",
        "    format=\"%(asctime)s [%(levelname)s]: %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger()\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # Increase limit to 16MB\n",
        "\n",
        "# Initialize the Magi model\n",
        "magi_model = AutoModel.from_pretrained(\n",
        "            \"ragavsachdeva/magiv2\", trust_remote_code=True).eval()\n",
        "\n",
        "def get_per_page_results(magi_model, chapter_pages, character_bank):\n",
        "  # Set to \"no_grad()\" so that there's inference without tracking gradients. Basically, this saves memory and computational resources by turning off gradient tracking.\n",
        "  with torch.no_grad():\n",
        "      per_page_results = magi_model.do_chapter_wide_prediction(\n",
        "          chapter_pages, character_bank, use_tqdm=True, do_ocr=True\n",
        "      )\n",
        "\n",
        "  return per_page_results\n",
        "\n",
        "def is_port_in_use(port):\n",
        "  with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "      return s.connect_ex((\"0.0.0.0\", port)) == 0\n",
        "\n",
        "def kill_process_on_port(port):\n",
        "  try:\n",
        "      # Find the process ID (PID) using netstat and taskkill\n",
        "      result = subprocess.check_output(\n",
        "          f\"netstat -ano | findstr :{port}\", shell=True\n",
        "      ).decode()\n",
        "      pid = int(result.strip().split()[-1])\n",
        "\n",
        "      # Kill the process\n",
        "      subprocess.call([\"taskkill\", \"/F\", \"/PID\", str(pid)])\n",
        "      print(f\"Port {port} is now free (process {pid} killed).\")\n",
        "  except subprocess.CalledProcessError:\n",
        "      print(f\"No process is using port {port}.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error freeing port {port}: {e}\")\n",
        "\n",
        "@app.route('/process-images-with-magi-model', methods=['POST'])\n",
        "def process_images_with_magi_model():\n",
        "  logger.info(\"Received request to /process-images-with-magi-model\")\n",
        "\n",
        "  try:\n",
        "    # Parse JSON payload\n",
        "    request_data = request.json\n",
        "    encoded_arrays = request_data.get(\"chapter_pages_image_numpy_array\")\n",
        "    character_bank = request_data.get(\"character_bank\")\n",
        "\n",
        "    # Decode and reconstruct the arrays\n",
        "    chapter_pages_image_numpy_array = [\n",
        "        np.frombuffer(base64.b64decode(item[\"data\"]), dtype=item[\"dtype\"]).reshape(item[\"shape\"])\n",
        "        for item in encoded_arrays\n",
        "    ]\n",
        "\n",
        "    # Run Magi model on the file\n",
        "    per_page_results = get_per_page_results(\n",
        "        magi_model, chapter_pages_image_numpy_array, character_bank\n",
        "    )\n",
        "\n",
        "    return per_page_results\n",
        "  except Exception as e:\n",
        "    return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/hello-world', methods=['GET'])\n",
        "def hello_world():\n",
        "  logger.info(\"Hello World endpoint hit\")\n",
        "  return 'Hello World!'\n",
        "\n",
        "def run_flask():\n",
        "  logger.info(\"Starting Flask server...\")\n",
        "  # Run Flask app without reloader (important for threading)\n",
        "  app.run(host=\"0.0.0.0\", port=port, debug=True, use_reloader=False)\n",
        "\n",
        "\n",
        "def force_free_port(port):\n",
        "    \"\"\"\n",
        "    Forcefully free a port by identifying and killing the process using it,\n",
        "    while avoiding critical processes like Colab kernel processes.\n",
        "    \"\"\"\n",
        "    for conn in psutil.net_connections(kind=\"inet\"):\n",
        "        if conn.laddr.port == port:\n",
        "            try:\n",
        "                # Get the process using the port\n",
        "                process = psutil.Process(conn.pid)\n",
        "                process_name = process.name().lower()\n",
        "                process_cmdline = \" \".join(process.cmdline())\n",
        "\n",
        "                # Skip Colab kernel-related processes\n",
        "                if \"colab\" in process_name or \"python\" in process_name and \"kernel\" in process_cmdline:\n",
        "                    print(f\"Skipping Colab or kernel-related process: {process_name} (PID {conn.pid})\")\n",
        "                    continue\n",
        "\n",
        "                # Kill the process\n",
        "                print(f\"Killing process using port {port}: {process_name} (PID {conn.pid})\")\n",
        "                os.kill(conn.pid, signal.SIGKILL)\n",
        "                print(f\"Successfully killed process {process_name} (PID {conn.pid}).\")\n",
        "            except psutil.NoSuchProcess:\n",
        "                print(f\"No such process exists for PID {conn.pid}.\")\n",
        "            except psutil.AccessDenied:\n",
        "                print(f\"Permission denied to kill process using port {port}: PID {conn.pid}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to kill process on port {port}: {e}\")\n",
        "            return\n",
        "    print(f\"Port {port} is not in use.\")\n",
        "\n",
        "def stop_all_tunnels():\n",
        "  try:\n",
        "    # Fetch all active tunnels\n",
        "    response = requests.get(f\"http://127.0.0.1:{port}/api/tunnels\")\n",
        "    tunnels = response.json().get(\"tunnels\", [])\n",
        "\n",
        "    # Terminate each tunnel\n",
        "    for tunnel in tunnels:\n",
        "        tunnel_name = tunnel[\"name\"]\n",
        "        delete_url = f\"http://127.0.0.1:{port}/api/tunnels/{tunnel_name}\"\n",
        "        requests.delete(delete_url)\n",
        "        print(f\"Terminated tunnel: {tunnel_name}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Failed to terminate tunnels: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  # Check and free the port if in use\n",
        "  force_free_port(port)\n",
        "\n",
        "  # Start Flask in a thread to prevent blocking\n",
        "  thread = threading.Thread(target=run_flask)\n",
        "  thread.start()\n",
        "\n",
        "  # Expose the Flask app to the internet using ngrok\n",
        "  public_url = ngrok.connect(port)\n",
        "  print(\"Public URL:\", public_url)\n",
        "\n",
        "  # Keep the Colab cell running\n",
        "  while True:\n",
        "      pass\n"
      ],
      "metadata": {
        "id": "spWkLnG1CATy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}